{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"VGOolXKczuN5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/hanliying/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","import pandas as pd\n","import time \n","import random\n","import json\n","import torch\n","from torch.nn.utils.rnn import pack_sequence\n","from torch.utils.data import DataLoader,Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24882,"status":"ok","timestamp":1664761697715,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"ipVtjcqp12Uz","outputId":"1a925e74-f823-4a41-f0e5-3a0cf5780f31"},"outputs":[],"source":["#for colab\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd /content/drive/My Drive/Pylon/test_examples/full_labeled_data"]},{"cell_type":"markdown","metadata":{},"source":["### Process Raw Data to Generate Primitive Activitiy Dataset\n","The data we use here is from the Third Nurse Care Activity Recognition Challenge. It contains 3-dimensional accelerometer data from 12 subjects performing 27 different activities. We have filtered out noisy data and stored them in 'data_full.json', but further preprocessing is necessary since each activity data has arbitrary length. We are going to segment each data with the same lenghth, and choose the activity classes with balanced training data size to be the primitive activities."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"44PWuZuJW65b"},"outputs":[],"source":["'''Read file and convert string to Python dict'''\n","\n","with open('./complex_event/data_full.json','r') as f:\n","  load_dict = json.load(f)\n","  # print(load_dict)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qLFVzetdOalU"},"outputs":[],"source":["'''Segment the raw data into same time length'''\n","\n","SEGMENT_SIZE = 60\n","SLIDING_SIZE = 5\n","\n","'''Choose primitive activitiy class indexed by target_label'''\n","\n","target_label = [4, 2, 12, 9]\n","label_map = {4:0, 2:1, 12:2, 9:3}\n","\n","n_class = len(target_label)\n","n_data_per_class = 2000"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25336,"status":"ok","timestamp":1664774080162,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"66n2hnt3cQ4I","outputId":"280b9972-e996-4a1a-88d1-edf9798327ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["171789\n","{0: 69316, 1: 72562, 3: 11705, 2: 18206}\n","Activity 4 :\n","The range of data length is ( 60 , 60 )\n","The distribution of data length is Counter({60: 69316})\n","Activity 2 :\n","The range of data length is ( 60 , 60 )\n","The distribution of data length is Counter({60: 72562})\n","Activity 9 :\n","The range of data length is ( 60 , 60 )\n","The distribution of data length is Counter({60: 11705})\n","Activity 12 :\n","The range of data length is ( 60 , 60 )\n","The distribution of data length is Counter({60: 18206})\n","{0: 498, 1: 503, 2: 488, 3: 511}\n","(6000, 3, 60)\n","(6000,)\n","(2000, 3, 60)\n","(2000,)\n"]}],"source":["'''Generate primitive activity data'''\n","\n","from collections import Counter\n","\n","x_prim_data = []\n","y_prim_data = []\n","time_period = set()\n","activity_len = {}\n","for item in load_dict:\n","  x, y, start, finish = item['acc'], item['old_label'], item['start'], item['finish']\n","  counter = dict(Counter(y_prim_data))\n","  if y in target_label and (start, finish) not in time_period and len(x) >= SEGMENT_SIZE:\n","    # Check duplicates\n","    time_period.add((start, finish))\n","    # Divide y into segements of length SEGMENT_SIZE\n","    # TODO: Here just discard segments with length smaller than SEGMENT_SIZE, we can use imputation later to keep them\n","    num = int((len(x) - SEGMENT_SIZE)/SLIDING_SIZE) + 1\n","    for i in range(num):\n","      x_i = x[i * SLIDING_SIZE:i * SLIDING_SIZE + SEGMENT_SIZE]\n","      # y_i = y\n","      y_i = label_map[y]\n","      x_prim_data.append(x_i)\n","      y_prim_data.append(y_i)\n","      \n","      # count the number of data with different length for each activity\n","      length = len(x_i)\n","      if y not in activity_len:\n","        activity_len[y] = [length]\n","      else:\n","        activity_len[y].append(length)\n","      \n","\n","\n","print(len(x_prim_data))\n","result = dict(Counter(y_prim_data))\n","print(result)\n","\n","\n","# count the number of data with different length for each activity\n","for key, val in activity_len.items():\n","  n = Counter(val)\n","  print(\"Activity\",key,\":\")\n","  print(\"The range of data length is (\", min(val), \",\", max(val), \")\")\n","  print(\"The distribution of data length is\", n)\n","\n","temp = list(zip(x_prim_data, y_prim_data))\n","random.shuffle(temp)\n","res1, res2 = zip(*temp)\n","# res1 and res2 come out as tuples, and so must be converted to lists.\n","x_prim_data, y_prim_data = list(res1), list(res2)\n","\n","x_prim_data = np.array(x_prim_data).transpose((0,2,1))\n","x_prim_data = x_prim_data.astype(np.float32)\n","y_prim_data = np.array(y_prim_data)\n","\n","# balance data set of different activities\n","x_prim_data_temp = np.zeros([n_data_per_class, x_prim_data.shape[1], x_prim_data.shape[2]])\n","y_prim_data_temp = np.zeros([n_data_per_class, 1])\n","for i in range(n_class):\n","  idx = np.where(y_prim_data==i)[0]\n","  idx = np.random.choice(idx, n_data_per_class, replace=False)\n","  if i == 0:\n","    x_prim_data_temp = x_prim_data[idx] \n","    y_prim_data_temp = y_prim_data[idx] \n","  else:\n","    x_prim_data_temp = np.concatenate([x_prim_data_temp,x_prim_data[idx]],axis=0)\n","    y_prim_data_temp = np.concatenate([y_prim_data_temp,y_prim_data[idx]],axis=0)\n","\n","x_prim_data = x_prim_data_temp\n","y_prim_data = y_prim_data_temp\n","\n","# shuffle again\n","temp = list(zip(x_prim_data, y_prim_data))\n","random.shuffle(temp)\n","res1, res2 = zip(*temp)\n","# res1 and res2 come out as tuples, and so must be converted to lists.\n","x_prim_data, y_prim_data = list(res1), list(res2)\n","\n","x_prim_data = np.array(x_prim_data)\n","x_prim_data.astype(np.float32)\n","y_prim_data = np.array(y_prim_data)\n","\n","# split some data to test set\n","x_prim_test = x_prim_data[-int(n_data_per_class/4*n_class):]\n","x_prim_train = x_prim_data[:-int(n_data_per_class/4*n_class)]\n","y_prim_test = y_prim_data[-int(n_data_per_class/4*n_class):]\n","y_prim_train = y_prim_data[:-int(n_data_per_class/4*n_class)]\n","\n","print(dict(Counter(y_prim_test)))\n","\n","print(x_prim_train.shape)\n","print(y_prim_train.shape)\n","print(x_prim_test.shape)\n","print(y_prim_test.shape)\n","\n","x_prim_train = torch.from_numpy(x_prim_train)\n","y_prim_train = torch.from_numpy(y_prim_train)\n","x_prim_test = torch.from_numpy(x_prim_test)\n","y_prim_test = torch.from_numpy(y_prim_test)\n","\n","# print(x_prim_train.shape)\n","# print(y_prim_train.shape)\n","# print(x_prim_test.shape)\n","# print(y_prim_test.shape)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1664774080162,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"A3HJjb23jk5x","outputId":"57482925-0a21-4d75-925f-ca8d311bfc20"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6000, 3, 60)\n","(2000, 3, 60)\n"]}],"source":["print(np.array(x_prim_train).shape)\n","print(np.array(x_prim_test).shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Neural Modules to Calssify Primitive Activity"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YRYm4W1Ga_kO"},"outputs":[],"source":["'''Define Neural Architecture'''\n","\n","from torch.nn.modules.activation import Softmax\n","from torch.nn.modules import dropout\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(self, n_class, drop_out=0.5):\n","      super().__init__()\n","      self.n_class = n_class\n","      self.conv = nn.Sequential(\n","          nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, padding='same'),\n","          nn.LeakyReLU(),\n","          nn.MaxPool1d(kernel_size=3, stride=3),\n","          nn.BatchNorm1d(num_features=8),\n","          nn.Dropout(drop_out),\n","\n","          # nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding='same'),\n","          # nn.LeakyReLU(),\n","          # nn.MaxPool1d(kernel_size=3, stride=3),\n","          # nn.BatchNorm1d(num_features=16),\n","          # nn.Dropout(drop_out)\n","      )\n","\n","      self.lstm = nn.LSTM(input_size=20, hidden_size=8, bidirectional=True, batch_first=True)\n","      self.fc = nn.Sequential(\n","          nn.LeakyReLU(),\n","          nn.Dropout(drop_out),\n","          nn.Flatten(),\n","          nn.Linear(in_features=128, out_features=self.n_class),\n","          nn.Softmax(dim=1)\n","      )\n","        \n","\n","    def forward(self, x):\n","      x = self.conv(x)\n","      # print(x.shape)\n","      x, _ = self.lstm(x)\n","      # print(x.shape)\n","      x = self.fc(x)\n","      # print(x.shape)\n","      return x\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Avl_JA3G5vBN"},"outputs":[{"data":{"text/plain":["\"\\ndef train(model, x_train, y_train, epoch, batch_size):\\n  batch_num = int(np.ceil(x_train.shape[0] / batch_size))\\n  for k in range(epoch):  # loop over the dataset multiple times\\n    running_loss = 0.0\\n    running_acc = 0.0\\n    for i in range(batch_num):\\n        # get the inputs; data is a list of [inputs, labels]\\n        if i == batch_num - 1:\\n          inputs, labels = x_train[i * batch_size:], y_train[i * batch_size:]\\n        else:\\n          inputs, labels = x_train[i * batch_size:(i + 1) * batch_size], y_train[i * batch_size:(i + 1) * batch_size]\\n        \\n        criterion = nn.CrossEntropyLoss()\\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\n        # zero the parameter gradients\\n        optimizer.zero_grad()\\n\\n        # forward + backward + optimize\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        # print statistics\\n        running_loss += loss.item()\\n        outputs = model(inputs)\\n        _, predicted = torch.max(outputs.data, 1)\\n        running_acc += (predicted == labels).sum().item() / batch_size\\n        if i % 100 == 99:    # print every 100 mini-batches\\n            print(f'[{k + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f} acc: {running_acc / 100:.3f}')\\n            running_loss = 0.0\\n            running_acc = 0.0\\n\\n  print('Finished Training')\\n\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","def train(model, x_train, y_train, epoch, batch_size):\n","  batch_num = int(np.ceil(x_train.shape[0] / batch_size))\n","  for k in range(epoch):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    for i in range(batch_num):\n","        # get the inputs; data is a list of [inputs, labels]\n","        if i == batch_num - 1:\n","          inputs, labels = x_train[i * batch_size:], y_train[i * batch_size:]\n","        else:\n","          inputs, labels = x_train[i * batch_size:(i + 1) * batch_size], y_train[i * batch_size:(i + 1) * batch_size]\n","        \n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        running_acc += (predicted == labels).sum().item() / batch_size\n","        if i % 100 == 99:    # print every 100 mini-batches\n","            print(f'[{k + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f} acc: {running_acc / 100:.3f}')\n","            running_loss = 0.0\n","            running_acc = 0.0\n","\n","  print('Finished Training')\n","'''"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"H1uju-St92n9"},"outputs":[],"source":["'''Evaluation function used for primitive activity classification'''\n","\n","def evaluate(model, x_test, y_test):\n","  correct = 0\n","  total = y_test.shape[0]\n","  with torch.no_grad():\n","    inputs, labels = x_test, y_test\n","    outputs = model(inputs)\n","    _, predict = torch.max(outputs.data, 1)\n","    correct = (predict == labels).sum().item() / total\n","\n","  print('Accuracy of the network on the test data: %d %%' % (\n","    100 * correct))\n","  \n","  class_correct = list(0. for i in range(model.n_class))\n","  class_total = list(0. for i in range(model.n_class))\n","  with torch.no_grad():\n","    inputs, labels = x_test, y_test\n","    outputs = model(inputs)\n","    _, predict = torch.max(outputs.data, 1)\n","    c = (predict == labels).squeeze()\n","    for i in range(labels.shape[0]):\n","      label = labels[i]\n","      class_correct[label] += c[i].item()\n","      class_total[label] += 1\n","\n","  print(class_total)\n","  for i in range(model.n_class):\n","    print('Accuracy of activity %2s : %2d %%' % (\n","    i, 100 * class_correct[i] / class_total[i]))\n","  return [predict, correct]\n"]},{"cell_type":"markdown","metadata":{},"source":["### Generate Complex Event Dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1664783273154,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"WyFfbzn5eGoa","outputId":"7d694549-bd83-4351-f720-6c38e01ebdb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([9600, 3, 180])\n","torch.Size([9600])\n","torch.Size([2400, 3, 180])\n","torch.Size([2400])\n","tensor([0, 1, 0,  ..., 0, 1, 0], dtype=torch.int32)\n"]}],"source":["''' Generate complex event training/test set using prmitive activities '''\n","\n","n_data_per_event = 6000\n","\n","''' \n","event_dict[key]: complex event class ID.\n","event_dict[item]: complex event pattern of the corresponding complex event class.\n","\n","In this simple example, event 0 means \"primitive activity 1 happens before 3\", event 1 means \"primitive activity 0 happens before 2\"\n","'''\n","event_dict = {0: [(1,0,3), (1,2,3), (1,3,0), (1,3,2)], 1: [(0,1,2), (0,3,2), (0,2,1), (0,2,3)]}\n","\n","\n","idx_list = []\n","for label in range(n_class):\n","    idx = list(np.where(y_prim_data == label)[0])\n","    idx_list.append(idx)\n","\n","x_event = np.empty((0,x_prim_data.shape[1],x_prim_data.shape[2]*3))\n","y_event = np.empty((0))\n","\n","for event_id, event_rules in event_dict.items():\n","    n_sub_event = len(event_rules)\n","    n_data_per_comb = int(n_data_per_event/n_sub_event)\n","\n","    for i in range(n_sub_event):\n","        event_rule = event_rules[i]\n","        e0, e1, e2 = event_rule[0], event_rule[1], event_rule[2]\n","        x_event_sub = np.concatenate((x_prim_data[np.random.choice(idx_list[e0], n_data_per_comb)],\n","                                      x_prim_data[np.random.choice(idx_list[e1], n_data_per_comb)],\n","                                      x_prim_data[np.random.choice(idx_list[e2], n_data_per_comb)]),\n","                                     axis=2)\n","        x_event = np.append(x_event, x_event_sub, axis=0)\n","        \n","    y_event = np.append(y_event, np.array([event_id] * n_data_per_event))\n","\n","\n","# shuffle the list\n","temp = list(zip(x_event, y_event))\n","random.shuffle(temp)\n","res1, res2 = zip(*temp)\n","# res1 and res2 come out as tuples, and so must be converted to lists.\n","x_event, y_event = np.array(res1), np.array(res2)\n","\n","# Construct trainig set and test set\n","x_event = x_event.astype(np.float32)\n","y_event = y_event.astype(np.int32)\n","\n","# x_event, y_event = utils.shuffle(x_event, y_event)\n","\n","n_event_data = len(x_event)\n","\n","x_event_train = x_event[:-int(n_event_data/5)]\n","x_event_test = x_event[-int(n_event_data/5):]\n","y_event_train = y_event[:-int(n_event_data/5)]\n","y_event_test = y_event[-int(n_event_data/5):]\n","\n","x_event_train = torch.from_numpy(x_event_train)\n","y_event_train = torch.from_numpy(y_event_train)\n","x_event_test = torch.from_numpy(x_event_test)\n","y_event_test = torch.from_numpy(y_event_test)\n","\n","print(x_event_train.shape)\n","print(y_event_train.shape)\n","print(x_event_test.shape)\n","print(y_event_test.shape)\n","\n","print(y_event_train)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1664774899089,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"L_vnJI7tRISP","outputId":"5ce20807-8a9d-4c17-eb75-ca8e1fef4c88"},"outputs":[{"name":"stdout","output_type":"stream","text":["(9600, 3, 180)\n","(2400, 3, 180)\n"]}],"source":["print(np.array(x_event_train).shape)\n","print(np.array(x_event_test).shape)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"1pcC_ncv393O"},"outputs":[],"source":["def predict_sequence(model, x_event, segment_size):\n","    '''\n","    model: the primitive activity classifer\n","    x_event: the complex event data of shape (batch_size, num_channel, sequence_length)\n","    segment_size: the length of the input required by primitive activity classifier\n","    '''\n","    segments_num = int(x_event.shape[-1] / segment_size)\n","    activity_sequence = np.zeros([x_event.shape[0],1])\n","    prob_sequence = np.zeros([x_event.shape[0],1])\n","    logits_sequence = []\n","\n","    for i in range(segments_num):\n","        x_event_temp = x_event[:,:,i * segment_size:(i + 1) * segment_size]\n","        # y_event_temp = y_event[i * segment_size:(i + 1) * segment_size]\n","        # print(x_event_temp.shape)\n","        outputs = model(x_event_temp)\n","        # prob, label = torch.max(outputs.data,1)\n","        # print(label.data.dtype)\n","        logits_sequence.append(outputs)\n","        # if i == 0:\n","            # activity_sequence = label.data.reshape(-1,1)\n","            # prob_sequence = prob.data.reshape(-1,1)\n","        # else:\n","        #     activity_sequence = np.concatenate([activity_sequence, label.data.reshape(-1,1)], axis=1)\n","        #     prob_sequence = np.concatenate([prob_sequence, prob.data.reshape(-1,1)], axis=1)\n","    \n","    return logits_sequence"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"uAOi7nxWUkcP"},"outputs":[],"source":["'''Define constraint function and Pylon constraint loss'''\n","\n","import sys\n","sys.path.append(\"../\")\n","\n","from pylon.constraint import constraint\n","from pylon.brute_force_solver import SatisfactionBruteForceSolver\n","# from pylon.brute_force_solver import *\n","\n","def complex_event(*logits_sequence, **kwargs):\n","    '''\n","    logits_sequence: a sequence of logits tensors returned by the primitive classifier\n","    kwargs['event_label']: ground truth complex event label\n","    kwargs['event_dict']: a dictionary recording the complex event class and corresponding patterns, event_dict[event_id] = [(event_pattern1), (event_pattern2), ...]\n","    '''\n","    for event_id, event_rules in kwargs['event_dict'].items():\n","        if logits_sequence in event_rules:\n","            return kwargs['event_label'] == event_id  # (torch,torch,torch) == (int,int,int)\n","    return False\n","\n","complex_event_cons = constraint(complex_event, SatisfactionBruteForceSolver())"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"vNIarV4fsach"},"outputs":[],"source":["model = CRNN(n_class)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1664781550564,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"3B4OAifXtlHv","outputId":"2d0c8aea-3f28-4780-8136-4d6abd030cdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 26 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 24 %\n","Accuracy of activity  1 :  5 %\n","Accuracy of activity  2 : 79 %\n","Accuracy of activity  3 :  2 %\n"]}],"source":["predict, acc = evaluate(model, x_prim_test, y_prim_test)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"elapsed":542,"status":"error","timestamp":1664783472928,"user":{"displayName":"LIYING HAN","userId":"12884455188191918408"},"user_tz":420},"id":"iZYmtAnhtsCz","outputId":"198cab53-077a-4683-a5c7-78aa3d683689"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/9600 [00:00<?, ?it/s]/Users/hanliying/Documents/UCLA/Research/Neurosym_proj/pylon-master/examples/../pylon/brute_force_solver.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  else torch.tensor(data=self.cond(*sample,**kwargs), dtype=torch.bool) for sample in samples ])\n"," 10%|█         | 1004/9600 [00:24<03:58, 36.03it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 24 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 20 %\n","Accuracy of activity  1 : 22 %\n","Accuracy of activity  2 : 59 %\n","Accuracy of activity  3 :  2 %\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 2006/9600 [00:50<03:22, 37.54it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 26 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 25 %\n","Accuracy of activity  1 : 46 %\n","Accuracy of activity  2 : 34 %\n","Accuracy of activity  3 :  2 %\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███▏      | 3005/9600 [01:13<03:00, 36.59it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 23 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 11 %\n","Accuracy of activity  1 : 80 %\n","Accuracy of activity  2 :  5 %\n","Accuracy of activity  3 :  1 %\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 4005/9600 [01:37<02:32, 36.65it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 25 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 :  6 %\n","Accuracy of activity  1 : 82 %\n","Accuracy of activity  2 :  0 %\n","Accuracy of activity  3 :  0 %\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 5003/9600 [02:08<02:42, 28.37it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 24 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 10 %\n","Accuracy of activity  1 : 85 %\n","Accuracy of activity  2 :  1 %\n","Accuracy of activity  3 :  1 %\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 6005/9600 [02:47<04:06, 14.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 25 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 12 %\n","Accuracy of activity  1 : 87 %\n","Accuracy of activity  2 :  0 %\n","Accuracy of activity  3 :  0 %\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 7005/9600 [03:25<01:24, 30.83it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 24 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 19 %\n","Accuracy of activity  1 : 80 %\n","Accuracy of activity  2 :  2 %\n","Accuracy of activity  3 :  1 %\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 8007/9600 [03:52<00:42, 37.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 27 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 15 %\n","Accuracy of activity  1 : 84 %\n","Accuracy of activity  2 :  3 %\n","Accuracy of activity  3 :  4 %\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 9003/9600 [04:18<00:16, 35.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 26 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 14 %\n","Accuracy of activity  1 : 83 %\n","Accuracy of activity  2 :  4 %\n","Accuracy of activity  3 :  2 %\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9600/9600 [04:34<00:00, 34.93it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the test data: 26 %\n","[498.0, 503.0, 488.0, 511.0]\n","Accuracy of activity  0 : 10 %\n","Accuracy of activity  1 : 80 %\n","Accuracy of activity  2 :  6 %\n","Accuracy of activity  3 :  6 %\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","NUM_EPOCHS = 1\n","\n","for epoch in range(NUM_EPOCHS):\n","      \n","    # train\n","    for i in tqdm(range(len(x_event_train))):\n","        model.train()\n","        optimizer.zero_grad()\n","        # print(len(x_event_train))\n","        logits_sequence = predict_sequence(model, x_event_train[i].unsqueeze(0), SEGMENT_SIZE)\n","\n","        closs = complex_event_cons(*logits_sequence, event_label=y_event_train[i], event_dict=event_dict)\n","\n","        closs.backward()\n","        optimizer.step()\n","        \n","        if i % 1000 == 0 and i != 0:\n","            evaluate(model, x_prim_test, y_prim_test)\n","        \n","    evaluate(model, x_prim_test, y_prim_test)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"2b2C4g2ntr1K"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["test = [1, 0, 3]\n","torch.tensor(test).unsqueeze(axis=-1)\n","(torch.tensor([0]), torch.tensor([2]), torch.tensor([1])) in [(0,1,1)]"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.12 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"7a719078278c4492d805b55cadf911a94c7633b48007fee7a7c47218b923b9ef"}}},"nbformat":4,"nbformat_minor":0}
